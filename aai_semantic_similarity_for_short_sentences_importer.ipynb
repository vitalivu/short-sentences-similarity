{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be621112",
   "metadata": {},
   "source": [
    "## Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98eab3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linhvt/Projects/github.com/vitalivu/sbert/py38/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:13:06] WARNING: ../src/gbm/gbtree.cc:348: Loading from a raw memory buffer on CPU only machine.  Changing predictor to auto.\n",
      "[17:13:06] WARNING: ../src/gbm/gbtree.cc:355: Loading from a raw memory buffer on CPU only machine.  Changing tree_method to hist.\n",
      "[17:13:06] WARNING: ../src/learner.cc:223: No visible GPU is found, setting `gpu_id` to -1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import io\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# by default, Pickle does not support load model to cpu\n",
    "class CpuUnpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)\n",
    "\n",
    "with open('xgboost_pair.pkl', \"rb\") as fIn:\n",
    "    stored_data = CpuUnpickler(fIn).load()\n",
    "    bst = stored_data['bst']\n",
    "    params = stored_data['params']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734a5ce",
   "metadata": {},
   "source": [
    "## Initialize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f423d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "df_train = pd.read_csv('../input/train.csv.zip')\n",
    "df_train.head()\n",
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "    \n",
    "eps = 5000 \n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "    \n",
    "def word_match_share(q1, q2):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(q1).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(q2).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    return (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "\n",
    "\n",
    "def tfidf_word_match_share(q1, q2):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(q1).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(q2).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    return np.sum(shared_weights) / np.sum(total_weights)\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sbert_model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "def cosine_sim(q1, q2):\n",
    "    embeddings1 = sbert_model.encode([clean_text(q1)], convert_to_tensor=True)\n",
    "    embeddings2 = sbert_model.encode([clean_text(q2)], convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    return cosine_scores[0][0].item()\n",
    "\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "cross_encoder = CrossEncoder('cross-encoder/quora-distilroberta-base')\n",
    "def cross_sim(q1, q2):\n",
    "    cross_scores = cross_encoder.predict([[clean_text(q1),clean_text(q2)]])\n",
    "    return cross_scores[0].item()\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower().replace(r'[^\\x00-\\x7f]', r' ')\n",
    "    for c in [\",\", \"!\", \".\", \"?\", '\"', \":\", \";\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\"]:\n",
    "        text = text.replace(c, \" \")\n",
    "    tokens = text.split(\" \")\n",
    "    tokens = [wnl.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in stops]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2b8d3",
   "metadata": {},
   "source": [
    "## Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0650f862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84312904]\n",
      "Took 0.1903 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "t0 = perf_counter()\n",
    "x_test = pd.DataFrame()\n",
    "\n",
    "q1 = 'How are falling prices of food articles balanced by an equitable increase in the farmer\\'s income in an economy?'\n",
    "q2 = 'How is the falling of prices of food articles balanced by an equitable increase in the farmer\\'s income in an economy?'\n",
    "x_test['word_match'] = [word_match_share(q1, q2)]\n",
    "x_test['tfidf_word_match'] = [tfidf_word_match_share(q1, q2)]\n",
    "x_test['cross_sim'] =  [cross_sim(q1, q2)]\n",
    "x_test['cosine_sim'] =  [cosine_sim(q1, q2)]\n",
    "d_test = xgb.DMatrix(x_test)\n",
    "\n",
    "y_est = bst.predict(d_test)\n",
    "t1 = perf_counter()\n",
    "print(y_est)\n",
    "print(\"Took {:.4f} seconds\".format(t1-t0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
