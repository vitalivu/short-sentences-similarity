{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vitalivu/short-sentences-similarity/blob/master/semantic_similarity_for_short_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPkAHfmowPYK"
   },
   "source": [
    "## SentenceBERT\n",
    "\n",
    "For original paper, see [arxiv.org](https://arxiv.org/abs/1908.10084)\n",
    "\n",
    "To work with this notebook, install with `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxEjg6aTwPYb",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vSF6btPwPYc"
   },
   "source": [
    "## Data\n",
    "This note nook using data from [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7A8wQSDrDn5j",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsHdAWanwPYd"
   },
   "source": [
    "### Running in Kaggle\n",
    "\n",
    "List the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66lTO9cMwPYe",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIt64cljwPYe",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/train.csv.zip', compression='zip', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhOKQbNewqcK"
   },
   "source": [
    "### Running in Colab\n",
    "\n",
    "In Colab, data stores in Google Drive. You have to upload your dataset manually to your google drive, then connect from this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTbMQJCFwPYc",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOhxf_oXwPYc"
   },
   "source": [
    "List the files, eg `data/quora/input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZ9xrwDqwPYd",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%ls /gdrive/MyDrive/Colab\\ Notebooks/data/quora/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cqcTzbhD0Xo"
   },
   "source": [
    "Get the file path from the previous command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTYQu01RwPYd",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/gdrive/MyDrive/Colab Notebooks/data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuiWCxwCD9Vq"
   },
   "source": [
    "Open example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXQ9D7DHwPYd",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/data/quora/input/train.csv.zip', compression='zip', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPAQe67zwPYe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Locally with Ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5i-RHBuDn5n",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('../data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNPuCRcODn5o",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/quora/input/train.csv.zip', compression='zip', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pJsGXDbwPYe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKb_dj2YwPYf",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "id_2_question_map = {}\n",
    "\n",
    "\n",
    "def add_to_map(key, val):\n",
    "    if key not in id_2_question_map:\n",
    "        id_2_question_map[key] = val\n",
    "\n",
    "\n",
    "def add_row_to_map(row):\n",
    "    add_to_map(row['qid1'], row['question1'])\n",
    "    add_to_map(row['qid2'], row['question2'])\n",
    "\n",
    "\n",
    "df.apply(lambda row: add_row_to_map(row), axis=1)\n",
    "\n",
    "len(id_2_question_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjs1IN7CwPYf"
   },
   "source": [
    "### Clean data\n",
    "\n",
    "- Lowercase original sentences\n",
    "- Remove some nonsense words, non-ASCII character\n",
    "- Replace with common phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Inflector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-6Yr-D-wPYf",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from inflector import Inflector\n",
    "from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words('english') # from nltk\n",
    "stop_words = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "# custom set of stopwords\n",
    "#stop_words = set(['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'which', 'while', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'])\n",
    "\n",
    "def clean_text(sent):\n",
    "    # Removing non ASCII chars\n",
    "    sent = str(sent).replace(r'[^\\x00-\\x7f]', r' ')\n",
    "    sent_norm = sent.lower()\n",
    "    # Remove any punctuation characters\n",
    "    for c in [\",\", \"!\", \".\", \"?\", \"'\", '\"', \":\", \";\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\"]:\n",
    "        sent_norm = sent_norm.replace(c, \" \")\n",
    "\n",
    "    # Remove stop words and Singularize all the words\n",
    "    tokens = sent_norm.split()\n",
    "    tokens = [Inflector().singularize(token) for token in tokens if token not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "clean_text('What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTXoGTOOwPYg"
   },
   "source": [
    "replace data with cleaned data: replace `question` with `clean_text(question)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0Vr2TcMwPYg",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_questions = []\n",
    "cleanidx_2_rawquestionid_map = {}\n",
    "\n",
    "for id, question in id_2_question_map.items():\n",
    "    q = clean_text(question)\n",
    "    \n",
    "    if q in clean_questions:\n",
    "        cleanidx_2_rawquestionid_map[clean_questions.index(q)].append(id)\n",
    "        continue\n",
    "        \n",
    "    clean_questions.append(q)\n",
    "    cleanidx_2_rawquestionid_map[len(clean_questions)-1] = [id]\n",
    "    \n",
    "print(\"raw questions size\", len(id_2_question_map))\n",
    "print(\"clean_questions size \",len(clean_questions))\n",
    "questions = np.array(clean_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6BECyamwPYh"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efWa5l3eDn5q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urKCk6y4Dn5q"
   },
   "source": [
    "### Create the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhAjnFwfwPYh",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "time_t1 = perf_counter()\n",
    "embeddings = model.encode(questions, convert_to_tensor=True)\n",
    "time_t2 = perf_counter()\n",
    "print(\"Computed sentence embeddings in {:.4f} seconds\".format(time_t2 - time_t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yra8LkrKwPYi"
   },
   "source": [
    "## Experiments\n",
    "Create a simple query and search for top 5 results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h3oqjPBwdnR"
   },
   "source": [
    "### Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPIypuEIwPYi",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import torch\n",
    "\n",
    "query = 'What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?'\n",
    "\n",
    "top_5 = min(5, len(embeddings))\n",
    "\n",
    "time_t1 = perf_counter()\n",
    "\n",
    "query_embedding = model.encode(clean_text(query), convert_to_tensor=True)\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
    "top_results = torch.topk(cos_scores, k=top_5)\n",
    "print(\"### Top 5 most similar queries of :\", query)\n",
    "for score, idx in zip(top_results[0], top_results[1]):\n",
    "    print(\"({:.4f})\".format(score), questions[idx])\n",
    "\n",
    "time_t2 = perf_counter()\n",
    "print(\"Compute consine-similarity in\", \"{:.4f}\".format(time_t2 - time_t1), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmSnEdM6wPYi"
   },
   "source": [
    "### Cross-Encoder\n",
    "\n",
    "Cannot run cross-encoder for the large dataset:\n",
    "- memory limitation,\n",
    "- computation ability and time-consuming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkRqX2Iewhkg"
   },
   "source": [
    "### Combination\n",
    "Using the top 100 in Bi-encoder to evaluate with Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qzBIVyvwPYj",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from time import perf_counter\n",
    "import torch\n",
    "\n",
    "query = 'What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?' # example from question1\n",
    "\n",
    "top_100 = min(100, len(embeddings))\n",
    "\n",
    "time_t1 = perf_counter()\n",
    "query_embedding = model.encode(clean_text(query), convert_to_tensor=True)\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
    "top_results = torch.topk(cos_scores, k=top_100)  # select top 100\n",
    "\n",
    "top_sentences = [questions[idx] for idx in zip(top_results[1])]  # extract top 100 sentences\n",
    "\n",
    "time_t2 = perf_counter()\n",
    "sentence_combinations = [[query, sentence] for sentence in top_sentences]\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/stsb-distilroberta-base')\n",
    "similarity_scores = cross_encoder.predict(sentence_combinations)\n",
    "sim_scores = reversed(np.argsort(similarity_scores))\n",
    "\n",
    "print(\"### Top 5 most similar queries of:\", query)\n",
    "for idx in [sim_score for _, sim_score in zip(range(5), sim_scores)]:\n",
    "    print(\"({:.4f}) {}\".format(similarity_scores[idx], top_sentences[idx]))\n",
    "\n",
    "time_t3 = perf_counter()\n",
    "print(\"Compute bi-encoder in\", \"{:.4f}\".format(time_t2 - time_t1), \"seconds\")\n",
    "print(\"Compute cross-encoder from top 100 in\", \"{:.4f}\".format(time_t3 - time_t2), \"seconds\")\n",
    "print(\"Total time: \", \"{:.4f}\".format(time_t3 - time_t1), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J9DfUWZwPYj"
   },
   "source": [
    "## Note and TODO\n",
    "Cannot apply to caculate for all sentences in both sets (memory not enough for 230TB =)) so:\n",
    "- we can apply one by one\n",
    "- a signmoi function: threshold for similarity scores to mark a question is similar or not\n",
    "    - linear regression to select the proper threshold\n",
    "- calculate the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64BPIBSmwPYj"
   },
   "source": [
    "## Export and import the model\n",
    "\n",
    "Export model to file. File can be used to restore model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ue5IKBs9wPYk",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Store sentences & embeddings on disc\n",
    "with open('/gdrive/MyDrive/Colab Notebooks/data/quora/output/embeddings_500k.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'questions': questions,\n",
    "                 'embeddings': embeddings},\n",
    "                fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcYRn3zAwPYk"
   },
   "source": [
    "Import model from file. In our case, kaggle generates model, then we use the pre-trained model to create the search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZuZlDabwPYk",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load sentences & embeddings from disc\n",
    "with open('/gdrive/MyDrive/Colab Notebooks/data/quora/output/embeddings_500k.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    questions = stored_data['sentences']\n",
    "    embeddings = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2jR2nxfDn5t"
   },
   "source": [
    "### Import from gpu model to cpu\n",
    "\n",
    "It's important to note that due to some limitation:\n",
    "- cannot host api server on kaggle/colab\n",
    "- cannot load the model from kaggle/colab to local machine (lack of GPU enough memory for model)\n",
    "\n",
    "So it's best to [load model trained with GPU to local machine with only CPU](https://stackoverflow.com/questions/57081727/load-pickle-file-obtained-from-gpu-to-cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZLbfNimDn5t",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "from time import perf_counter\n",
    "import pickle\n",
    "import torch\n",
    "import io\n",
    "\n",
    "\n",
    "# by default, Pickle does not support load model to cpu\n",
    "class CpuUnpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)\n",
    "\n",
    "\n",
    "# model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "t1 = perf_counter()\n",
    "with open('../data/quora/output/embeddings', \"rb\") as fIn:\n",
    "    stored_data = CpuUnpickler(fIn).load()\n",
    "    questions = stored_data['questions']\n",
    "    embeddings = stored_data['embeddings']\n",
    "t2 = perf_counter()\n",
    "\n",
    "print(\"Took {:.2f} seconds to import model\".format(t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuDnEpt6Dn5u"
   },
   "source": [
    "## Evaluating model\n",
    "\n",
    "### Only bi-encoder => accuracy\n",
    "### Only cross-encoder => computation issue\n",
    "\n",
    "### New model\n",
    "[Formular1 - kaggle](https://www.kaggle.com/plarmuseau/semantic-similarity-for-short-sentences)\n",
    "\n",
    "[Word order similarity - paper](https://arxiv.org/pdf/1802.05667.pdf)\n",
    "```\n",
    "P = 0.85\n",
    "simi = P * sematic_similarity(q1, q2, is_duplicate) + (1-P)*word_order_similarity(q1, q2)\n",
    "```\n",
    "\n",
    "- S1: `A gem is a jewel or stone that is used in jewellery.`\n",
    "- S2: `A jewel is a precious stone used to decorate valuable things that you wear, such as rings or necklaces.`\n",
    "\n",
    "|Words|Similarity|\n",
    "|--|--|\n",
    "|jewel - jewel |0.997421032224|\n",
    "|jewel - stone| 0.217431543606|\n",
    "|jewel - used| 0.0|\n",
    "|jewel - decorate| 0.0|\n",
    "|jewel - valuable| 0.0|\n",
    "|jewel - things| 0.406309448212|\n",
    "|jewel - wear| 0.0|\n",
    "|jewel - rings| 0.456849659596|\n",
    "|jewel - necklaces| 0.41718607131|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnyWQUYdedyg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "QPkAHfmowPYK",
    "GsHdAWanwPYd",
    "sPAQe67zwPYe"
   ],
   "include_colab_link": true,
   "name": "semantic-similarity-for-short-sentences.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
