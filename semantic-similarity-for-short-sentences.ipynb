{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## SentenceBERT\n",
    "\n",
    "Install with `pip`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "This note nook using data from [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs)\n",
    "\n",
    "### Read data\n",
    "Data store in GDrive:\n",
    "\n",
    "#### Connect to google drive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### List the files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%ls /gdrive/MyDrive/Colab\\ Notebooks/data/quora/input/train.csv.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('/gdrive/MyDrive/Colab Notebooks/data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/data/quora/input/train.csv.zip', compression='zip', sep=',')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question1 = df['question1'].unique()\n",
    "question1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean data\n",
    "\n",
    "- Lowercase original sentences\n",
    "- Remove some nonsense words, non-ASCII character\n",
    "- Replace with common phrases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stopwords = set(['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'which', 'while', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'])\n",
    "\n",
    "\n",
    "def cleantext(sent):\n",
    "    # Removing non ASCII chars\n",
    "    sent = str(sent).replace(r'[^\\x00-\\x7f]',r' ')\n",
    "\n",
    "    # Replace some common paraphrases\n",
    "    sent_norm = sent.lower()\\\n",
    "        .replace(\"how do you\", \"how do i\")\\\n",
    "        .replace(\"how do we\", \"how do i\")\\\n",
    "        .replace(\"how can we\", \"how can i\")\\\n",
    "        .replace(\"how can you\", \"how can i\")\\\n",
    "        .replace(\"how can i\", \"how do i\")\\\n",
    "        .replace(\"really true\", \"true\")\\\n",
    "        .replace(\"what are the importance\", \"what is the importance\")\\\n",
    "        .replace(\"what was\", \"what is\")\\\n",
    "        .replace(\"so many\", \"many\")\\\n",
    "        .replace(\"would it take\", \"will it take\")\n",
    "\n",
    "    # Remove any punctuation characters\n",
    "    for c in [\",\", \"!\", \".\", \"?\", \"'\", '\"', \":\", \";\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\"]:\n",
    "        sent_norm = sent_norm.replace(c, \" \")\n",
    "\n",
    "    # Remove stop words\n",
    "    tokens = sent_norm.split()\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "cleantext('What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "then, replace data with cleaned data: replace `question` with `cleantext(question)`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question1 = df['question1'].unique()\n",
    "question1 = np.array(list(map(cleantext, question1)))\n",
    "question1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question2 = df['question2'].unique()\n",
    "question2 = np.array(list(map(cleantext, question2)))\n",
    "question2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models\n",
    "### Create the embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "startTime = perf_counter()\n",
    "embeddings1 = model.encode(question1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(question2, convert_to_tensor=True)\n",
    "endTime = perf_counter()\n",
    "print(\"Computed sentence embeddings in {:.4f} seconds\".format(endTime - startTime))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Experiments\n",
    "Create a simple query and search for top 10 results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import torch\n",
    "\n",
    "queries = ['What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?'] # example from question1\n",
    "\n",
    "top_5 = min(5, len(embeddings2))\n",
    "\n",
    "time_t1 = perf_counter()\n",
    "for query in queries:\n",
    "    query_embedding = model.encode(cleantext(query), convert_to_tensor=True)\n",
    "    cos_scores = util.pytorch_cos_sim(query_embedding, embeddings2)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_5)\n",
    "    print(\"### Query:\", query)\n",
    "    print(\"Top 5 most similar queries:\")\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(\"({:.4f})\".format(score), question2[idx])\n",
    "\n",
    "time_t2 = perf_counter()\n",
    "print(\"Compute consine-similarity in\",\"{:.4f}\".format(time_t2 - time_t1),\"seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the top 100 in Bi-encoder to evaluate with Cross-Encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from time import perf_counter\n",
    "import torch\n",
    "\n",
    "query = 'What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?' # example from question1\n",
    "\n",
    "top_100 = min(100, len(embeddings2))\n",
    "\n",
    "time_t1 = perf_counter()\n",
    "query_embedding = model.encode(cleantext(query), convert_to_tensor=True)\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, embeddings2)[0]\n",
    "top_results = torch.topk(cos_scores, k=top_100) # select top 100\n",
    "\n",
    "top_sentences = [ question2[idx] for idx in zip(top_results[1])] # extract top 100 sentences\n",
    "\n",
    "time_t2 = perf_counter()\n",
    "sentence_combinations = [[query, sentence] for sentence in top_sentences]\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/stsb-distilroberta-base')\n",
    "similarity_scores = cross_encoder.predict(sentence_combinations)\n",
    "sim_scores = reversed(np.argsort(similarity_scores))\n",
    "\n",
    "print(\"### Query:\", query)\n",
    "print(\"Top 5 most similar queries:\")\n",
    "for idx in [sim_score for _,sim_score in zip(range(5), sim_scores)]:\n",
    "    print(\"({:.4f}) {}\".format(similarity_scores[idx], top_sentences[idx]))\n",
    "\n",
    "time_t3 = perf_counter()\n",
    "print(\"Compute bi-encoder in\",\"{:.4f}\".format(time_t2 - time_t1),\"seconds\")\n",
    "print(\"Compute cross-encoder from top 100 in\",\"{:.4f}\".format(time_t3 - time_t2),\"seconds\")\n",
    "print(\"Total time: \", \"{:.4f}\".format(time_t3 - time_t1), \"seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Note and TODO\n",
    "Cannot apply to caculate for all sentences in both sets (memory not enough for 230TB =)) so:\n",
    "- we can apply one by one\n",
    "- a signmoi function: threshold for similarity scores to mark a question is similar or not\n",
    "    - linear regression to select the proper threshold\n",
    "- calculate the accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export and import the model\n",
    "\n",
    "Export model to file. File can be used to restore model later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Store sentences & embeddings on disc\n",
    "with open('question1.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': question1, 'embeddings': embeddings1}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('question2.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': question2, 'embeddings': embeddings2}, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import model from file. In our case, kaggle generates model, then we use the pre-trained model to create the search engine."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Load sentences & embeddings from disc\n",
    "with open('question1.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    question1 = stored_data['sentences']\n",
    "    embeddings1 = stored_data['embeddings']\n",
    "with open('question2.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    question2 = stored_data['sentences']\n",
    "    embeddings2 = stored_data['embeddings']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}