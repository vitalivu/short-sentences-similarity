{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPkAHfmowPYK"
   },
   "source": [
    "## SentenceBERT\n",
    "\n",
    "For original paper, see [arxiv.org](https://arxiv.org/abs/1908.10084)\n",
    "\n",
    "To work with this notebook, install with `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxEjg6aTwPYb",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vSF6btPwPYc"
   },
   "source": [
    "## Data\n",
    "This note nook using data from [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.2.4-cp38-cp38-manylinux1_x86_64.whl (9.7 MB)\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/linhvt/Projects/github.com/vitalivu/sbert/py38/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/linhvt/Projects/github.com/vitalivu/sbert/py38/lib/python3.8/site-packages (from pandas) (1.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/linhvt/Projects/github.com/vitalivu/sbert/py38/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.2.4 pytz-2021.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linhvt/Projects/github.com/vitalivu/sbert/py38/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsHdAWanwPYd"
   },
   "source": [
    "### Running in Kaggle\n",
    "\n",
    "List the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66lTO9cMwPYe",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIt64cljwPYe",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/train.csv.zip', compression='zip', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhOKQbNewqcK"
   },
   "source": [
    "### Running in Colab\n",
    "\n",
    "In Colab, data stores in Google Drive. You have to upload your dataset manually to your google drive, then connect from this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTbMQJCFwPYc",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOhxf_oXwPYc"
   },
   "source": [
    "List the files, eg `data/quora/input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZ9xrwDqwPYd",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%ls /gdrive/MyDrive/Colab\\ Notebooks/data/quora/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTYQu01RwPYd",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/gdrive/MyDrive/Colab Notebooks/data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXQ9D7DHwPYd",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/data/quora/input/train.csv.zip', compression='zip', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPAQe67zwPYe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Locally with Ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('../data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/quora/input/train.csv.zip', compression='zip', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pJsGXDbwPYe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKb_dj2YwPYf",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question1 = df['question1'].unique()\n",
    "question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question1.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjs1IN7CwPYf"
   },
   "source": [
    "### Clean data\n",
    "\n",
    "- Lowercase original sentences\n",
    "- Remove some nonsense words, non-ASCII character\n",
    "- Replace with common phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3-6Yr-D-wPYf",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what approx annual cost living studying uic chicago indian student'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'which', 'while', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'])\n",
    "\n",
    "\n",
    "def cleantext(sent):\n",
    "    # Removing non ASCII chars\n",
    "    sent = str(sent).replace(r'[^\\x00-\\x7f]',r' ')\n",
    "\n",
    "    # Replace some common paraphrases\n",
    "    sent_norm = sent.lower()\\\n",
    "        .replace(\"how do you\", \"how do i\")\\\n",
    "        .replace(\"how do we\", \"how do i\")\\\n",
    "        .replace(\"how can we\", \"how can i\")\\\n",
    "        .replace(\"how can you\", \"how can i\")\\\n",
    "        .replace(\"how can i\", \"how do i\")\\\n",
    "        .replace(\"really true\", \"true\")\\\n",
    "        .replace(\"what are the importance\", \"what is the importance\")\\\n",
    "        .replace(\"what was\", \"what is\")\\\n",
    "        .replace(\"so many\", \"many\")\\\n",
    "        .replace(\"would it take\", \"will it take\")\n",
    "\n",
    "    # Remove any punctuation characters\n",
    "    for c in [\",\", \"!\", \".\", \"?\", \"'\", '\"', \":\", \";\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\"]:\n",
    "        sent_norm = sent_norm.replace(c, \" \")\n",
    "\n",
    "    # Remove stop words\n",
    "    tokens = sent_norm.split()\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "cleantext('What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTXoGTOOwPYg"
   },
   "source": [
    "replace data with cleaned data: replace `question` with `cleantext(question)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0Vr2TcMwPYg",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question1 = df['question1'].unique()\n",
    "question1 = np.array(list(map(cleantext, question1)))\n",
    "question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ue0FW2mOwPYh",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question2 = df['question2'].unique()\n",
    "question2 = np.array(list(map(cleantext, question2)))\n",
    "question2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6BECyamwPYh"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhAjnFwfwPYh",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "startTime = perf_counter()\n",
    "embeddings1 = model.encode(question1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(question2, convert_to_tensor=True)\n",
    "endTime = perf_counter()\n",
    "print(\"Computed sentence embeddings in {:.4f} seconds\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yra8LkrKwPYi"
   },
   "source": [
    "## Experiments\n",
    "Create a simple query and search for top 5 results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h3oqjPBwdnR"
   },
   "source": [
    "### Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LPIypuEIwPYi",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Query: What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?\n",
      "Top 5 most similar queries:\n",
      "(0.6720) what cost living (monthly yearly) graduate student studying mit\n",
      "(0.6302) how much indian student earn studying masters degree uk\n",
      "(0.6255) what minimum living expenses per month dubai student\n",
      "(0.6064) how much would masters mis cost indian student nyu living education included\n",
      "(0.6030) how much earning international student make per hour oslo norway working cafes bars restaurants\n",
      "Compute consine-similarity in 0.5140 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "import torch\n",
    "\n",
    "queries = ['What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?'] # example from question1\n",
    "\n",
    "top_5 = min(5, len(embeddings2))\n",
    "\n",
    "time_t1 = perf_counter()\n",
    "for query in queries:\n",
    "    query_embedding = model.encode(cleantext(query), convert_to_tensor=True)\n",
    "    cos_scores = util.pytorch_cos_sim(query_embedding, embeddings2)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_5)\n",
    "    print(\"### Query:\", query)\n",
    "    print(\"Top 5 most similar queries:\")\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(\"({:.4f})\".format(score), question2[idx])\n",
    "\n",
    "time_t2 = perf_counter()\n",
    "print(\"Compute consine-similarity in\",\"{:.4f}\".format(time_t2 - time_t1),\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmSnEdM6wPYi"
   },
   "source": [
    "### Cross-Encoder\n",
    "\n",
    "Cannot run cross-encoder for the large dataset:\n",
    "- memory limitation,\n",
    "- computation ability and time-consuming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkRqX2Iewhkg"
   },
   "source": [
    "### Combination\n",
    "Using the top 100 in Bi-encoder to evaluate with Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8qzBIVyvwPYj",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Query: What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?\n",
      "Top 5 most similar queries:\n",
      "(0.6741) what cost living (monthly yearly) graduate student studying mit\n",
      "(0.5357) what approximated cost attending norwegian university indian student opting undergraduate programs\n",
      "(0.5315) what cost living denver (co) student\n",
      "(0.5217) what opportunity cost studying university\n",
      "(0.4945) how much would masters mis cost indian student nyu living education included\n",
      "Compute bi-encoder in 0.5010 seconds\n",
      "Compute cross-encoder from top 100 in 12.6645 seconds\n",
      "Total time:  13.1655 seconds\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from time import perf_counter\n",
    "import torch\n",
    "\n",
    "query = 'What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?' # example from question1\n",
    "\n",
    "top_100 = min(100, len(embeddings2))\n",
    "\n",
    "time_t1 = perf_counter()\n",
    "query_embedding = model.encode(cleantext(query), convert_to_tensor=True)\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, embeddings2)[0]\n",
    "top_results = torch.topk(cos_scores, k=top_100) # select top 100\n",
    "\n",
    "top_sentences = [ question2[idx] for idx in zip(top_results[1])] # extract top 100 sentences\n",
    "\n",
    "time_t2 = perf_counter()\n",
    "sentence_combinations = [[query, sentence] for sentence in top_sentences]\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/stsb-distilroberta-base')\n",
    "similarity_scores = cross_encoder.predict(sentence_combinations)\n",
    "sim_scores = reversed(np.argsort(similarity_scores))\n",
    "\n",
    "print(\"### Query:\", query)\n",
    "print(\"Top 5 most similar queries:\")\n",
    "for idx in [sim_score for _,sim_score in zip(range(5), sim_scores)]:\n",
    "    print(\"({:.4f}) {}\".format(similarity_scores[idx], top_sentences[idx]))\n",
    "\n",
    "time_t3 = perf_counter()\n",
    "print(\"Compute bi-encoder in\",\"{:.4f}\".format(time_t2 - time_t1),\"seconds\")\n",
    "print(\"Compute cross-encoder from top 100 in\",\"{:.4f}\".format(time_t3 - time_t2),\"seconds\")\n",
    "print(\"Total time: \", \"{:.4f}\".format(time_t3 - time_t1), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J9DfUWZwPYj"
   },
   "source": [
    "## Note and TODO\n",
    "Cannot apply to caculate for all sentences in both sets (memory not enough for 230TB =)) so:\n",
    "- we can apply one by one\n",
    "- a signmoi function: threshold for similarity scores to mark a question is similar or not\n",
    "    - linear regression to select the proper threshold\n",
    "- calculate the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64BPIBSmwPYj"
   },
   "source": [
    "## Export and import the model\n",
    "\n",
    "Export model to file. File can be used to restore model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ue5IKBs9wPYk",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Store sentences & embeddings on disc\n",
    "with open('question1.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': question1, 'embeddings': embeddings1}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('question2.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': question2, 'embeddings': embeddings2}, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcYRn3zAwPYk"
   },
   "source": [
    "Import model from file. In our case, kaggle generates model, then we use the pre-trained model to create the search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZuZlDabwPYk",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load sentences & embeddings from disc\n",
    "with open('question1.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    question1 = stored_data['sentences']\n",
    "    embeddings1 = stored_data['embeddings']\n",
    "with open('question2.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    question2 = stored_data['sentences']\n",
    "    embeddings2 = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from gpu model to cpu\n",
    "\n",
    "It's important to note that due to some limitation:\n",
    "- cannot host api server on kaggle/colab\n",
    "- cannot load the model from kaggle/colab to local machine (lack of GPU enough memory for model)\n",
    "\n",
    "So it's best to [load model trained with GPU to local machine with only CPU](https://stackoverflow.com/questions/57081727/load-pickle-file-obtained-from-gpu-to-cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 4.57 seconds to import model\n"
     ]
    }
   ],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "from time import perf_counter\n",
    "import pickle\n",
    "import torch\n",
    "import io\n",
    "\n",
    "\n",
    "class CpuUnpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else: return super().find_class(module, name)\n",
    "\n",
    "        \n",
    "# model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "t1 = perf_counter()\n",
    "#Load sentences & embeddings from disc\n",
    "with open('embeddings.pkl', \"rb\") as fIn:\n",
    "    stored_data = CpuUnpickler(fIn).load()\n",
    "    question1 = stored_data['sentences']\n",
    "    embeddings1 = stored_data['embeddings']\n",
    "    question2 = stored_data['sentences2']\n",
    "    embeddings2 = stored_data['embeddings2']\n",
    "    \n",
    "t2 = perf_counter()\n",
    "\n",
    "print(\"Took {:.2f} seconds to import model\".format(t2-t1))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "semantic-similarity-for-short-sentences.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
