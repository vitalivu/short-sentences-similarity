{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semantic-similarity-for-short-sentences.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QPkAHfmowPYK",
        "GsHdAWanwPYd",
        "sPAQe67zwPYe"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitalivu/short-sentences-similarity/blob/master/semantic_similarity_for_short_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPkAHfmowPYK"
      },
      "source": [
        "## SentenceBERT\n",
        "\n",
        "For original paper, see [arxiv.org](https://arxiv.org/abs/1908.10084)\n",
        "\n",
        "To work with this notebook, install with `pip`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxEjg6aTwPYb",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": true
      },
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install pandas\n",
        "!pip install lzma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vSF6btPwPYc"
      },
      "source": [
        "## Data\n",
        "This note nook using data from [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7A8wQSDrDn5j"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsHdAWanwPYd"
      },
      "source": [
        "### Running in Kaggle\n",
        "\n",
        "List the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66lTO9cMwPYe",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIt64cljwPYe",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "df = pd.read_csv('../input/train.csv.zip', compression='zip', sep=',')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhOKQbNewqcK"
      },
      "source": [
        "### Running in Colab\n",
        "\n",
        "In Colab, data stores in Google Drive. You have to upload your dataset manually to your google drive, then connect from this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTbMQJCFwPYc",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOhxf_oXwPYc"
      },
      "source": [
        "List the files, eg `data/quora/input`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ9xrwDqwPYd",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "%ls /gdrive/MyDrive/Colab\\ Notebooks/data/quora/input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cqcTzbhD0Xo"
      },
      "source": [
        "Get the file path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTYQu01RwPYd",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "for dirname, _, filenames in os.walk('/gdrive/MyDrive/Colab Notebooks/data'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuiWCxwCD9Vq"
      },
      "source": [
        "Open example data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXQ9D7DHwPYd",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "df = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/data/quora/input/train.csv.zip', compression='zip', sep=',')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPAQe67zwPYe",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Locally with Ubuntu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LXjrboipDn5n"
      },
      "source": [
        "!sudo apt-get install liblzma-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "V5i-RHBuDn5n"
      },
      "source": [
        "for dirname, _, filenames in os.walk('../data'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "VNPuCRcODn5o"
      },
      "source": [
        "df = pd.read_csv('../data/quora/input/train.csv.zip', compression='zip', sep=',')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pJsGXDbwPYe",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Example data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKb_dj2YwPYf",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "id_2_question_map = {}\n",
        "\n",
        "def add_to_map(key, val):\n",
        "    if key not in id_2_question_map:\n",
        "        id_2_question_map[key] = val\n",
        "    \n",
        "\n",
        "def add_row(row):\n",
        "    add_to_map(row['qid1'], row['question1'])\n",
        "    add_to_map(row['qid2'], row['question2'])\n",
        "\n",
        "df.apply(lambda row: add_row(row), axis=1)\n",
        "\n",
        "len(id_2_question_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Gv6Jza04Dn5o"
      },
      "source": [
        "len(set(id_2_question_map.values()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjs1IN7CwPYf"
      },
      "source": [
        "### Clean data\n",
        "\n",
        "- Lowercase original sentences\n",
        "- Remove some nonsense words, non-ASCII character\n",
        "- Replace with common phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-6Yr-D-wPYf",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "#stopwords = set(['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'which', 'while', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'])\n",
        "\n",
        "\n",
        "def clean_text(sent):\n",
        "    # Removing non ASCII chars\n",
        "    sent = str(sent).replace(r'[^\\x00-\\x7f]',r' ')\n",
        "\n",
        "    # Replace some common paraphrases\n",
        "    sent_norm = sent.lower()\\\n",
        "        .replace(\"how do you\", \"how do i\")\\\n",
        "        .replace(\"how do we\", \"how do i\")\\\n",
        "        .replace(\"how can we\", \"how can i\")\\\n",
        "        .replace(\"how can you\", \"how can i\")\\\n",
        "        .replace(\"how can i\", \"how do i\")\\\n",
        "        .replace(\"really true\", \"true\")\\\n",
        "        .replace(\"what are the importance\", \"what is the importance\")\\\n",
        "        .replace(\"what was\", \"what is\")\\\n",
        "        .replace(\"so many\", \"many\")\\\n",
        "        .replace(\"would it take\", \"will it take\")\n",
        "\n",
        "    # Remove any punctuation characters\n",
        "    for c in [\",\", \"!\", \".\", \"?\", \"'\", '\"', \":\", \";\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\"]:\n",
        "        sent_norm = sent_norm.replace(c, \" \")\n",
        "\n",
        "    # Remove stop words\n",
        "    tokens = sent_norm.split()\n",
        "    tokens = [token for token in tokens if token not in stopwords]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "clean_text('What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTXoGTOOwPYg"
      },
      "source": [
        "replace data with cleaned data: replace `question` with `clean_text(question)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0Vr2TcMwPYg",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "questions = np.array(list(filter(None, set(map(clean_text, set(id_2_question_map.values()))))))\n",
        "questions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFTxZlC4QLc-"
      },
      "source": [
        "questions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6BECyamwPYh"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "efWa5l3eDn5q"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urKCk6y4Dn5q"
      },
      "source": [
        "### Create the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhAjnFwfwPYh",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "from time import perf_counter\n",
        "\n",
        "time_t1 = perf_counter()\n",
        "embeddings = model.encode(questions, convert_to_tensor=True)\n",
        "time_t2 = perf_counter()\n",
        "print(\"Computed sentence embeddings in {:.4f} seconds\".format(time_t2 - time_t1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yra8LkrKwPYi"
      },
      "source": [
        "## Experiments\n",
        "Create a simple query and search for top 5 results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h3oqjPBwdnR"
      },
      "source": [
        "### Bi-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPIypuEIwPYi",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "from time import perf_counter\n",
        "import torch\n",
        "\n",
        "queries = ['What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?'] # example from question1\n",
        "\n",
        "top_5 = min(5, len(embeddings))\n",
        "\n",
        "time_t1 = perf_counter()\n",
        "for query in queries:\n",
        "    query_embedding = model.encode(clean_text(query), convert_to_tensor=True)\n",
        "    cos_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_5)\n",
        "    print(\"### Query:\", query)\n",
        "    print(\"Top 5 most similar queries:\")\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        print(\"({:.4f})\".format(score), questions[idx])\n",
        "\n",
        "time_t2 = perf_counter()\n",
        "print(\"Compute consine-similarity in\",\"{:.4f}\".format(time_t2 - time_t1),\"seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmSnEdM6wPYi"
      },
      "source": [
        "### Cross-Encoder\n",
        "\n",
        "Cannot run cross-encoder for the large dataset:\n",
        "- memory limitation,\n",
        "- computation ability and time-consuming\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkRqX2Iewhkg"
      },
      "source": [
        "### Combination\n",
        "Using the top 100 in Bi-encoder to evaluate with Cross-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qzBIVyvwPYj",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from time import perf_counter\n",
        "import torch\n",
        "\n",
        "query = 'What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?' # example from question1\n",
        "\n",
        "top_100 = min(100, len(embeddings))\n",
        "\n",
        "time_t1 = perf_counter()\n",
        "query_embedding = model.encode(clean_text(query), convert_to_tensor=True)\n",
        "cos_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
        "top_results = torch.topk(cos_scores, k=top_100) # select top 100\n",
        "\n",
        "top_sentences = [ questions[idx] for idx in zip(top_results[1])] # extract top 100 sentences\n",
        "\n",
        "time_t2 = perf_counter()\n",
        "sentence_combinations = [[query, sentence] for sentence in top_sentences]\n",
        "\n",
        "cross_encoder = CrossEncoder('cross-encoder/stsb-distilroberta-base')\n",
        "similarity_scores = cross_encoder.predict(sentence_combinations)\n",
        "sim_scores = reversed(np.argsort(similarity_scores))\n",
        "\n",
        "print(\"### Query:\", query)\n",
        "print(\"Top 5 most similar queries:\")\n",
        "for idx in [sim_score for _,sim_score in zip(range(5), sim_scores)]:\n",
        "    print(\"({:.4f}) {}\".format(similarity_scores[idx], top_sentences[idx]))\n",
        "\n",
        "time_t3 = perf_counter()\n",
        "print(\"Compute bi-encoder in\",\"{:.4f}\".format(time_t2 - time_t1),\"seconds\")\n",
        "print(\"Compute cross-encoder from top 100 in\",\"{:.4f}\".format(time_t3 - time_t2),\"seconds\")\n",
        "print(\"Total time: \", \"{:.4f}\".format(time_t3 - time_t1), \"seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J9DfUWZwPYj"
      },
      "source": [
        "## Note and TODO\n",
        "Cannot apply to caculate for all sentences in both sets (memory not enough for 230TB =)) so:\n",
        "- we can apply one by one\n",
        "- a signmoi function: threshold for similarity scores to mark a question is similar or not\n",
        "    - linear regression to select the proper threshold\n",
        "- calculate the accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64BPIBSmwPYj"
      },
      "source": [
        "## Export and import the model\n",
        "\n",
        "Export model to file. File can be used to restore model later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue5IKBs9wPYk",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "import pickle\n",
        "\n",
        "#Store sentences & embeddings on disc\n",
        "with open('/gdrive/MyDrive/Colab Notebooks/data/quora/output/embeddings_500k.pkl', \"wb\") as fOut:\n",
        "    pickle.dump({'questions': questions, \n",
        "                 'embeddings': embeddings}, \n",
        "                fOut, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcYRn3zAwPYk"
      },
      "source": [
        "Import model from file. In our case, kaggle generates model, then we use the pre-trained model to create the search engine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZuZlDabwPYk",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "source": [
        "#Load sentences & embeddings from disc\n",
        "with open('question1.pkl', \"rb\") as fIn:\n",
        "    stored_data = pickle.load(fIn)\n",
        "    question1 = stored_data['sentences']\n",
        "    embeddings1 = stored_data['embeddings']\n",
        "with open('question2.pkl', \"rb\") as fIn:\n",
        "    stored_data = pickle.load(fIn)\n",
        "    question2 = stored_data['sentences']\n",
        "    embeddings2 = stored_data['embeddings']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2jR2nxfDn5t"
      },
      "source": [
        "### Import from gpu model to cpu\n",
        "\n",
        "It's important to note that due to some limitation:\n",
        "- cannot host api server on kaggle/colab\n",
        "- cannot load the model from kaggle/colab to local machine (lack of GPU enough memory for model)\n",
        "\n",
        "So it's best to [load model trained with GPU to local machine with only CPU](https://stackoverflow.com/questions/57081727/load-pickle-file-obtained-from-gpu-to-cpu)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "sZLbfNimDn5t"
      },
      "source": [
        "# from sentence_transformers import SentenceTransformer, util\n",
        "from time import perf_counter\n",
        "import pickle\n",
        "import torch\n",
        "import io\n",
        "\n",
        "# by default, Pickle does not support load model to cpu\n",
        "class CpuUnpickler(pickle.Unpickler):\n",
        "    def find_class(self, module, name):\n",
        "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
        "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
        "        else: return super().find_class(module, name)\n",
        "\n",
        "        \n",
        "# model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "\n",
        "t1 = perf_counter()\n",
        "#Load sentences & embeddings from disc\n",
        "with open('embeddings.pkl', \"rb\") as fIn:\n",
        "    stored_data = CpuUnpickler(fIn).load()\n",
        "    question1 = stored_data['sentences']\n",
        "    embeddings1 = stored_data['embeddings']\n",
        "    question2 = stored_data['sentences2']\n",
        "    embeddings2 = stored_data['embeddings2']\n",
        "    \n",
        "t2 = perf_counter()\n",
        "\n",
        "print(\"Took {:.2f} seconds to import model\".format(t2-t1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuDnEpt6Dn5u"
      },
      "source": [
        "## Evaluating model\n",
        "\n",
        "### Only bi-encoder => accuracy\n",
        "### Only cross-encoder => computation issue\n",
        "\n",
        "### New model\n",
        "[Formular1 - kaggle](https://www.kaggle.com/plarmuseau/semantic-similarity-for-short-sentences)\n",
        "\n",
        "[Word order similarity - paper](https://arxiv.org/pdf/1802.05667.pdf)\n",
        "```\n",
        "P = 0.85\n",
        "simi = P * sematic_similarity(q1, q2, is_duplicate) + (1-P)*word_order_similarity(q1, q2)\n",
        "```\n",
        "\n",
        "- S1: `A gem is a jewel or stone that is used in jewellery.`\n",
        "- S2: `A jewel is a precious stone used to decorate valuable things that you wear, such as rings or necklaces.`\n",
        "\n",
        "|Words|Similarity|\n",
        "|--|--|\n",
        "|jewel - jewel |0.997421032224|\n",
        "|jewel - stone| 0.217431543606|\n",
        "|jewel - used| 0.0|\n",
        "|jewel - decorate| 0.0|\n",
        "|jewel - valuable| 0.0|\n",
        "|jewel - things| 0.406309448212|\n",
        "|jewel - wear| 0.0|\n",
        "|jewel - rings| 0.456849659596|\n",
        "|jewel - necklaces| 0.41718607131|\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnyWQUYdedyg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}